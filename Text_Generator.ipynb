{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing dependencies"
      ],
      "metadata": {
        "id": "nU6fF6FLI12z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Dropout, Dense, LSTM\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFdTRxb5Jy2E",
        "outputId": "2ac5c981-8a1a-4f92-a772-b3ab7435148e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting and organizing data"
      ],
      "metadata": {
        "id": "lznCn-u3J-9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization means separating the words, phrases or meaningful items."
      ],
      "metadata": {
        "id": "RGCmMScAxEiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading data\n",
        "data = open('Frankenstein.txt').read()"
      ],
      "metadata": {
        "id": "358HA3tJKC93"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization and standardization\n",
        "def tokenize_words(input):\n",
        "  #converting everything into lowercase to standardize the data\n",
        "    input = input.lower()\n",
        "    #instantiating the tokenizer which in this case is the RegexpTokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    #tokenizing text\n",
        "    tokens = tokenizer.tokenize(input)\n",
        "    #filtering stopwords because they do not have any meaning like 'a' or 'the'\n",
        "    #the list of stopwords is from the natural language toolkit module or nltk\n",
        "    filtered = filter(lambda token : token not in stopwords.words('english'), tokens)\n",
        "    return \"\".join(filtered)\n",
        "#function calling\n",
        "processed_inputs = tokenize_words(data)"
      ],
      "metadata": {
        "id": "dlU6AkHCKPLZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. converting the words into numbers\n",
        "2. sorting the words \n",
        "3. set is used so that only one type of the same word remains in the output\n",
        "4. creating a dictionary to store the words as keys and their representative numbers as values. "
      ],
      "metadata": {
        "id": "w28aif6RCr5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#characters to numbers\n",
        "chars = sorted(list(set(processed_inputs)))\n",
        "char_to_num = dict((c,i) for i,c in enumerate(chars))"
      ],
      "metadata": {
        "id": "7TV0sLnsMSOd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if words to chars or chars to num has worked\n",
        "input_len = len(processed_inputs)\n",
        "vocab_len = len(chars)\n",
        "print(\"Total number of characters:\", input_len)\n",
        "print(\"Total vocab:\", vocab_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkPq9WqPBnGN",
        "outputId": "7c04ce65-4430-4f3a-f7a1-4dae80a96f40"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 220931\n",
            "Total vocab: 37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#seq length\n",
        "#we're defining the length of an individual sequence which is th mapping of input characters as integers\n",
        "seq_length = 100\n",
        "x_data = []\n",
        "y_data = []"
      ],
      "metadata": {
        "id": "LyKAPpPYDJEQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loop through the sequence\n",
        "#going through the sequence and creating a bunch of sequences which start with the next character un the sequence\n",
        "for i in range(0, input_len - seq_length, 1):\n",
        "  #defining input and output sequences\n",
        "  #input sequences are the current character and desired sequence length\n",
        "    in_seq = processed_inputs[i:i + seq_length]\n",
        "#output sequences are the initial character and desired sequence length\n",
        "    out_seq = processed_inputs[i + seq_length]\n",
        "#converting list of characters to integers based on previous values and appending the values to our lists\n",
        "    x_data.append([char_to_num[char] for char in in_seq])\n",
        "    y_data.append(char_to_num[out_seq])\n",
        "    #checking to see how many input sequences we have in total\n",
        "n_patterns = len(x_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "Gpi8-H9eDZBD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert input sequence to np array that our network can use\n",
        "X = np.reshape(x_data, (n_patterns, seq_length, 1))\n",
        "X = X/float(vocab_len)\n",
        "# one hot - encoding with binary numbers for our label data\n",
        "Y = np_utils.to_categorical(y_data)"
      ],
      "metadata": {
        "id": "6_3zvLfXyPct"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the model\n",
        "#dropout layers are used to prevent overfitting\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense( Y.shape[1], activation='softmax'))"
      ],
      "metadata": {
        "id": "flg59rbN89ST"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "metadata": {
        "id": "iNE3y_gO-lSU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving weights\n",
        "filepath = 'model_weights_saved.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
        "desired_callbacks = [checkpoint]"
      ],
      "metadata": {
        "id": "5H7lf2WO-0TR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit model and let it train on the data\n",
        "#more the epoch greater the accuracy\n",
        "model.fit(X, Y, epochs = 4, batch_size=256, callbacks = desired_callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZAlqUun_M10",
        "outputId": "e1c9f299-9858-426b-e759-b4bbb4235b34"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "863/863 [==============================] - ETA: 0s - loss: 2.9243\n",
            "Epoch 1: loss improved from inf to 2.92425, saving model to model_weights_saved.hdf5\n",
            "863/863 [==============================] - 3359s 4s/step - loss: 2.9243\n",
            "Epoch 2/4\n",
            "863/863 [==============================] - ETA: 0s - loss: 2.9051\n",
            "Epoch 2: loss improved from 2.92425 to 2.90512, saving model to model_weights_saved.hdf5\n",
            "863/863 [==============================] - 3311s 4s/step - loss: 2.9051\n",
            "Epoch 3/4\n",
            "863/863 [==============================] - ETA: 0s - loss: 2.8914\n",
            "Epoch 3: loss improved from 2.90512 to 2.89138, saving model to model_weights_saved.hdf5\n",
            "863/863 [==============================] - 3318s 4s/step - loss: 2.8914\n",
            "Epoch 4/4\n",
            "863/863 [==============================] - ETA: 0s - loss: 2.8576\n",
            "Epoch 4: loss improved from 2.89138 to 2.85760, saving model to model_weights_saved.hdf5\n",
            "863/863 [==============================] - 3353s 4s/step - loss: 2.8576\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f75fdae8d10>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#recompile model with same weights\n",
        "filename = 'model_weights_saved.hdf5'\n",
        "model.load_weights(filename)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "metadata": {
        "id": "TDPoItdKIe1k"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting output of the model back into characters\n",
        "num_to_char = dict((i,c)for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "IgJ3AeSQI2Ws"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#random seed to generate text and test the model\n",
        "start = np.random.randint(0, len(x_data)-1)\n",
        "pattern = x_data[start]\n",
        "print(\"Random Seed: \")\n",
        "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB18D7PuJIks",
        "outputId": "73397940-f443-4e35-c30d-94ab0fa1d20e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed: \n",
            "\" tedframefeverishcheekssawchangealsothinnerlostmuchheavenlyvivacitycharmedgentlenesssoftlookscompassi \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate text\n",
        "for i in range (1000):\n",
        "  x = np.reshape(pattern, (1,len(pattern), 1))\n",
        "  x = x/float(vocab_len)\n",
        "  prediction = model.predict(x, verbose = 0)\n",
        "  index = np.argmax(prediction)\n",
        "  result= num_to_char[index]\n",
        "  seq_in = [num_to_char[value] for value in pattern]\n",
        "  sys.stdout.write(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnK0SoMf1tpB",
        "outputId": "0b76a2b0-8112-4bb9-fecf-17b1ebd7e6da"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "erereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerereerer"
          ]
        }
      ]
    },
  ]
}
